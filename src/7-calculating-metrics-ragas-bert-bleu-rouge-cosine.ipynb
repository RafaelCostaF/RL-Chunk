{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a43ed581-dc81-41d4-8f0e-3ec0d8f21719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 32 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from llmFunctions import *\n",
    "from pandarallel import pandarallel\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize pandarallel\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8595cd9-9dc0-4533-9f23-49fde4ad3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# It has been done with the other methods too (RL, Random, BM25 ,Faiss, etc.)\n",
    "df = pd.read_parquet(\"df_with_predicted_responses_faiss.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46860379-5981-4464-add2-480798cd3fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a5f986234d47c4b3e619e67c898f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=24), Label(value='0 / 24'))), HBox…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Use parallel_apply instead of progress_apply\n",
    "df[\"llm_response_faiss\"] = df.parallel_apply(\n",
    "    lambda row: clean_response(row[\"llm_response_faiss\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Ensure all data is string-typed\n",
    "df = df.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c33325-80e9-4160-9657-2c4f2b5e3746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE BLEU BERT COSINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e592e08-2bf0-44ea-a974-330b36a457ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rafael/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from llmFunctions import *\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import bert_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# ------------------ Helper Functions ------------------ #\n",
    "\n",
    "def preprocess(df, generated_response_column, gold_column):\n",
    "    \"\"\"Lowercase and extract text from response.\"\"\"\n",
    "    df['llm_response_text'] = df[generated_response_column]\n",
    "    df['clean_answer'] = df[gold_column].str.lower()\n",
    "    df['llm_response_text'] = df['llm_response_text'].str.lower()\n",
    "    return df\n",
    "\n",
    "def compute_bleu(reference, hypothesis):\n",
    "    reference_tokens = reference.split()\n",
    "    hypothesis_tokens = hypothesis.split()\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference_tokens], hypothesis_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "\n",
    "def compute_rouge_l(reference, hypothesis):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, hypothesis)\n",
    "    return scores['rougeL'].fmeasure\n",
    "\n",
    "def compute_spacy_cosine_similarity(reference, hypothesis):\n",
    "    doc1 = nlp(reference)\n",
    "    doc2 = nlp(hypothesis)\n",
    "    if doc1.vector_norm and doc2.vector_norm:\n",
    "        return 1 - cosine(doc1.vector, doc2.vector)\n",
    "    return 0.0\n",
    "\n",
    "def compute_metrics(df):\n",
    "    df['BLEU-4'] = df.apply(lambda row: compute_bleu(row['clean_answer'], row['llm_response_text']), axis=1)\n",
    "    df['ROUGE-L F1'] = df.apply(lambda row: compute_rouge_l(row['clean_answer'], row['llm_response_text']), axis=1)\n",
    "    df['Cosine Sim'] = df.apply(lambda row: compute_spacy_cosine_similarity(row['clean_answer'], row['llm_response_text']), axis=1)\n",
    "    \n",
    "    # Compute BERTScore\n",
    "    P, R, F1 = bert_score.score(df['llm_response_text'].tolist(), df['clean_answer'].tolist(), lang=\"en\", verbose=True)\n",
    "    df['BERTScore F1'] = F1.numpy()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def display_final_table(df):\n",
    "    print(\"\\nFinal Table with Metrics:\")\n",
    "    print(df[['answer', 'llm_response_text','ROUGE-L F1', 'BLEU-4',  'BERTScore F1', 'Cosine Sim']].round(4))\n",
    "\n",
    "def group_by_domain(df):\n",
    "    domain_means = df.groupby(\"domain\")[['ROUGE-L F1', 'BLEU-4',  'BERTScore F1', 'Cosine Sim']].mean()\n",
    "    print(\"\\nMédia das métricas por domínio:\")\n",
    "    print(domain_means.round(4))\n",
    "\n",
    "def group_by_domain_and_algo(df):\n",
    "    grouped = df.groupby([\"domain\", \"algo\"])[['ROUGE-L F1', 'BLEU-4',  'BERTScore F1', 'Cosine Sim']].mean()\n",
    "    print(\"\\nMédia das métricas por domínio e algoritmo:\")\n",
    "    print(grouped.round(4))\n",
    "\n",
    "# ------------------ Main Execution ------------------ #\n",
    "\n",
    "def evaluate_llm_responses(df, generated_response_column='llm_response', gold_column='answer'):\n",
    "    df = preprocess(df, generated_response_column, gold_column)\n",
    "    df = compute_metrics(df)\n",
    "    display_final_table(df)\n",
    "    group_by_domain(df)\n",
    "    group_by_domain_and_algo(df)\n",
    "    return df\n",
    "\n",
    "# ------------------ Usage ------------------ #\n",
    "\n",
    "# Example:\n",
    "# df = pd.read_csv(\"your_file.csv\")\n",
    "# df = evaluate_llm_responses(df, generated_response_column='llm_response', gold_column='answer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a573df39-35d2-4984-951c-a1c0630e4ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_idx', 'algo', 'actions', 'chunks_selected', 'total_reward',\n",
       "       'steps', 'interaction_id', 'domain', 'question_type',\n",
       "       'static_or_dynamic', 'query', 'answer', 'page_results_text',\n",
       "       'llm_response', 'llm_response_text', 'BLEU-4', 'ROUGE-L F1',\n",
       "       'Cosine Sim', 'BERTScore F1', 'faiss_chunks_selected',\n",
       "       'llm_response_faiss'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c0f17-ce22-461b-925e-d05b81c5e524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0c0d67e11d466a9b384e281ec318b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ee65fb633341e8b0c67846e3b94129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 11.44 seconds, 65.75 sentences/sec\n",
      "\n",
      "Final Table with Metrics:\n",
      "                                                answer  \\\n",
      "0    the total value of all etfs in the global mark...   \n",
      "1    the total value of all etfs in the global mark...   \n",
      "2    the total value of all etfs in the global mark...   \n",
      "3    the total value of all etfs in the global mark...   \n",
      "4    the company with the highest ratio of insider ...   \n",
      "..                                                 ...   \n",
      "322  alex essoe, amanda fuller, fabianne therese, n...   \n",
      "328                                                 44   \n",
      "329                                                 44   \n",
      "330                                                 44   \n",
      "331                                                 44   \n",
      "\n",
      "                                     llm_response_text  ROUGE-L F1  BLEU-4  \\\n",
      "0    the provided text appears to be instructions f...      0.0000  0.0000   \n",
      "1    based on the provided information, there is no...      0.0000  0.0000   \n",
      "2    the provided instruction is clear. if the answ...      0.0000  0.0000   \n",
      "3    the provided instruction is clear. if the answ...      0.0000  0.0000   \n",
      "4    the provided instruction states that if there'...      0.1277  0.0214   \n",
      "..                                                 ...         ...     ...   \n",
      "322  alexandra essoe, amanda fuller, noah segan, fa...      0.4615  0.1543   \n",
      "328  the provided instruction is clear. if the answ...      0.0000  0.0000   \n",
      "329  the provided instruction states that if there'...      0.0000  0.0000   \n",
      "330  the provided instruction states that if there'...      0.0000  0.0000   \n",
      "331  based on the provided information, the `llm_re...      0.0000  0.0000   \n",
      "\n",
      "     BERTScore F1  Cosine Sim  \n",
      "0          0.8108      0.6094  \n",
      "1          0.8259      0.6136  \n",
      "2          0.8184      0.6094  \n",
      "3          0.8080      0.6322  \n",
      "4          0.7864      0.8258  \n",
      "..            ...         ...  \n",
      "322        0.9169      0.9856  \n",
      "328        0.7837      0.2832  \n",
      "329        0.8071      0.2893  \n",
      "330        0.7943      0.2705  \n",
      "331        0.8066      0.2704  \n",
      "\n",
      "[752 rows x 6 columns]\n",
      "\n",
      "Média das métricas por domínio:\n",
      "         ROUGE-L F1  BLEU-4  BERTScore F1  Cosine Sim\n",
      "domain                                               \n",
      "finance      0.1448  0.0258        0.8303      0.4529\n",
      "movie        0.2245  0.0672        0.8583      0.6419\n",
      "music        0.3536  0.1251        0.8880      0.6752\n",
      "open         0.4088  0.1331        0.8756      0.6527\n",
      "sports       0.3208  0.1227        0.8667      0.6021\n",
      "\n",
      "Média das métricas por domínio e algoritmo:\n",
      "                       ROUGE-L F1  BLEU-4  BERTScore F1  Cosine Sim\n",
      "domain  algo                                                       \n",
      "finance ddpg               0.1337  0.0134        0.8314      0.4414\n",
      "        ppo                0.1460  0.0336        0.8353      0.4571\n",
      "        recurrent_ppo      0.1241  0.0116        0.8150      0.4520\n",
      "        sac                0.1798  0.0455        0.8410      0.4606\n",
      "movie   ddpg               0.2386  0.0672        0.8594      0.6517\n",
      "        ppo                0.2508  0.0869        0.8631      0.6522\n",
      "        recurrent_ppo      0.1925  0.0425        0.8505      0.6166\n",
      "        sac                0.2137  0.0713        0.8603      0.6476\n",
      "music   ddpg               0.3589  0.1385        0.8918      0.6606\n",
      "        ppo                0.3741  0.1271        0.8925      0.7064\n",
      "        recurrent_ppo      0.3359  0.1032        0.8766      0.6376\n",
      "        sac                0.3429  0.1328        0.8913      0.6957\n",
      "open    ddpg               0.4046  0.1108        0.8754      0.6425\n",
      "        ppo                0.4260  0.1400        0.8797      0.6555\n",
      "        recurrent_ppo      0.3544  0.1270        0.8678      0.6240\n",
      "        sac                0.4577  0.1571        0.8803      0.6950\n",
      "sports  ddpg               0.2974  0.1032        0.8614      0.5708\n",
      "        ppo                0.3630  0.1525        0.8725      0.6124\n",
      "        recurrent_ppo      0.2781  0.1106        0.8579      0.5859\n",
      "        sac                0.3437  0.1221        0.8753      0.6432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "eval_df = evaluate_llm_responses(df, \"llm_response_faiss\", \"clean_answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7895c734-0dce-476c-88ae-ac60a57290a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ragas Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df72439-81c9-40cf-b663-1950d65c5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from typing import Union, List\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"api_key_here\"\n",
    "\n",
    "# Initialize LLM and Embeddings wrappers\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "def safe_eval(x):\n",
    "    \"\"\"Safely parse stringified lists.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except Exception:\n",
    "        return [str(x)]\n",
    "    \n",
    "    \n",
    "\n",
    "def evaluate_ragas(\n",
    "    df: pd.DataFrame,\n",
    "    answer_col: str,\n",
    "    generated_answer_col: str,\n",
    "    context_col: str,\n",
    "    query_col: str,\n",
    "    groupby_col: Union[str, List[str]] = None,  # compatible with Python <3.10\n",
    ") -> dict:\n",
    "    def _prepare_and_evaluate(group_df):\n",
    "        group_df = group_df[[answer_col, generated_answer_col, context_col, query_col]].copy()\n",
    "        group_df[context_col] = group_df[context_col].apply(safe_eval)\n",
    "\n",
    "        df_ragas = group_df.rename(columns={\n",
    "            answer_col: \"answer\",\n",
    "            generated_answer_col: \"generated_answer\",\n",
    "            context_col: \"retrieved_contexts\",\n",
    "            query_col: \"user_input\"\n",
    "        })\n",
    "        df_ragas[\"reference\"] = df_ragas[\"answer\"]\n",
    "\n",
    "        dataset = Dataset.from_pandas(df_ragas)\n",
    "\n",
    "        return evaluate(\n",
    "            dataset,\n",
    "            metrics=[\n",
    "                faithfulness,\n",
    "                answer_relevancy,\n",
    "                answer_correctness,\n",
    "                context_precision,\n",
    "                context_recall,\n",
    "            ],\n",
    "            llm=evaluator_llm,\n",
    "            embeddings=evaluator_embeddings,\n",
    "        )\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    if groupby_col:\n",
    "        if isinstance(groupby_col, str):\n",
    "            groupby_col = [groupby_col]\n",
    "        for group_keys, group_df in df.groupby(groupby_col):\n",
    "            group_name = \"__\".join(str(k) for k in group_keys) if isinstance(group_keys, tuple) else str(group_keys)\n",
    "            try:\n",
    "                results[group_name] = _prepare_and_evaluate(group_df)\n",
    "            except Exception as e:\n",
    "                results[group_name] = {\"error\": str(e)}\n",
    "    else:\n",
    "        results[\"overall\"] = _prepare_and_evaluate(df)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef40e29-94e0-44d6-88f2-3a0bda2227f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = evaluate_ragas(\n",
    "    df,\n",
    "    answer_col=\"clean_answer\",\n",
    "    generated_answer_col=\"llm_response_faiss\",\n",
    "    context_col=\"chunks_selected_faiss\",\n",
    "    query_col=\"query\",\n",
    "    groupby_col=[\"algo\", \"domain\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ee5332e-8212-4720-9d80-ba9c7af4d22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ddpg__finance': {'faithfulness': 0.2656, 'answer_relevancy': 0.7935, 'answer_correctness': 0.9406, 'context_precision': 0.1547, 'context_recall': 0.5114},\n",
       " 'ddpg__movie': {'faithfulness': 0.3077, 'answer_relevancy': 0.7815, 'answer_correctness': 0.9101, 'context_precision': 0.1459, 'context_recall': 0.4460},\n",
       " 'ddpg__music': {'faithfulness': 0.4775, 'answer_relevancy': 0.8324, 'answer_correctness': 0.9284, 'context_precision': 0.2112, 'context_recall': 0.5673},\n",
       " 'ddpg__open': {'faithfulness': 0.6423, 'answer_relevancy': 0.8142, 'answer_correctness': 0.9787, 'context_precision': 0.3391, 'context_recall': 0.7092},\n",
       " 'ddpg__sports': {'faithfulness': 0.3611, 'answer_relevancy': 0.7699, 'answer_correctness': 0.9687, 'context_precision': 0.2143, 'context_recall': 0.6445},\n",
       " 'ppo__finance': {'faithfulness': 0.3083, 'answer_relevancy': 0.7919, 'answer_correctness': 0.8970, 'context_precision': 0.2378, 'context_recall': 0.4523},\n",
       " 'ppo__movie': {'faithfulness': 0.3171, 'answer_relevancy': 0.7832, 'answer_correctness': 0.9273, 'context_precision': 0.2285, 'context_recall': 0.6535},\n",
       " 'ppo__music': {'faithfulness': 0.4333, 'answer_relevancy': 0.8367, 'answer_correctness': 0.9512, 'context_precision': 0.2257, 'context_recall': 0.5791},\n",
       " 'ppo__open': {'faithfulness': 0.5455, 'answer_relevancy': 0.8135, 'answer_correctness': 0.9881, 'context_precision': 0.3692, 'context_recall': 0.6583},\n",
       " 'ppo__sports': {'faithfulness': 0.3465, 'answer_relevancy': 0.7705, 'answer_correctness': 0.9714, 'context_precision': 0.2540, 'context_recall': 0.5944},\n",
       " 'recurrent_ppo__finance': {'faithfulness': 0.2814, 'answer_relevancy': 0.7908, 'answer_correctness': 0.9490, 'context_precision': 0.1787, 'context_recall': 0.4590},\n",
       " 'recurrent_ppo__movie': {'faithfulness': 0.2583, 'answer_relevancy': 0.7796, 'answer_correctness': 0.9362, 'context_precision': 0.1405, 'context_recall': 0.4999},\n",
       " 'recurrent_ppo__music': {'faithfulness': 0.3838, 'answer_relevancy': 0.8331, 'answer_correctness': 0.9375, 'context_precision': 0.1636, 'context_recall': 0.5009},\n",
       " 'recurrent_ppo__open': {'faithfulness': 0.5227, 'answer_relevancy': 0.8135, 'answer_correctness': 0.9568, 'context_precision': 0.3143, 'context_recall': 0.6981},\n",
       " 'recurrent_ppo__sports': {'faithfulness': 0.3889, 'answer_relevancy': 0.7491, 'answer_correctness': 1.0000, 'context_precision': 0.1841, 'context_recall': 0.6867},\n",
       " 'sac__finance': {'faithfulness': 0.2667, 'answer_relevancy': 0.7867, 'answer_correctness': 0.9092, 'context_precision': 0.2504, 'context_recall': 0.5439},\n",
       " 'sac__movie': {'faithfulness': 0.2824, 'answer_relevancy': 0.7784, 'answer_correctness': 0.9301, 'context_precision': 0.2018, 'context_recall': 0.5268},\n",
       " 'sac__music': {'faithfulness': 0.4216, 'answer_relevancy': 0.8340, 'answer_correctness': 0.9449, 'context_precision': 0.2371, 'context_recall': 0.5928},\n",
       " 'sac__open': {'faithfulness': 0.5429, 'answer_relevancy': 0.8158, 'answer_correctness': 0.9858, 'context_precision': 0.4530, 'context_recall': 0.7155},\n",
       " 'sac__sports': {'faithfulness': 0.5108, 'answer_relevancy': 0.7652, 'answer_correctness': 0.9766, 'context_precision': 0.2944, 'context_recall': 0.6647}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192e532-5020-41e3-b5c6-28cac6163d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e785a0d-d3bb-4a80-a9c4-9f1a8ef02b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
